#!/usr/bin/env python

import numpy as np
import pickle
import torch
from torch import nn
from torch import optim
from torch.nn import functional as F

def load_dataset(fn):
    """
    Load the dataset from "fn" (generated by mkdataset.py and modified by gethandpos.py)

    returns the list of all data
    """

    # read index file
    with open(fn + '.index.pickle', 'rb') as f:
        index = pickle.load(f)

    # load part data according to index file
    data = []
    for item in index:
        # read part data ...
        with open(item['fn'], 'rb') as f:
            part_data = pickle.load(f)
        # ... and validate its correctness
        if len(part_data) != item['len']:
            print('Warning: incorrect index file: len({}) != {}'.format(item['fn'], item['len']))
        
        # load all data into exactly one list
        data.extend(part_data)
    return data

# load testset
test = load_dataset('testset')

print('test:', len(test))

def normalize_hand(hand):
    """
    Normalize detected hand keypoints, moving the center (mean of coord) of non-zero points to (0,0)
    """
    retv = hand.astype(np.float)
    nonzero = (retv == 0).sum(1) != 2
    retv[nonzero] -= np.average(retv[nonzero], 0)
    return retv




import torchvision.models as models

class mynet(nn.Module):
    """
    The naive FCN model
    """
    def __init__(self, shp, l):
        super(mynet, self).__init__()
        self.thenet = nn.Sequential(
            nn.Linear(np.product(shp), 128), nn.ReLU(True),
            nn.Linear(128, 256), nn.ReLU(True),
            nn.Linear(256, 72), nn.ReLU(True),
            nn.Linear(72, l),
            )
    def forward(self, X):
        v = X.flatten(start_dim=1)
        v = self.thenet(v)
        return v

#### load pretrained model and the number-str mapping of label
obj = torch.load('train-result.pth')
Y_map = obj['Y_map']
EPOCH = obj['epoch']

#### initialize the model and optimizer
# net = models.densenet161()
net = mynet(test[0]['hand'].shape, len(Y_map))
loss = nn.CrossEntropyLoss()
opt = optim.Adagrad(net.parameters(), lr = 0.01)

opt.load_state_dict(obj['opt'])
net.load_state_dict(obj['net'])

print('Model at epoch #{}'.format(EPOCH))


def __linear_gradient(pos, c1, c2):
    """
    linear interpolation for numbers

    pos: number
      a real number in [0, 1]
    c1: number
    c2: number
    """
    return ((c2 - c1) * pos + c1 if pos > 0 else c1) if pos < 1 else c2

def linear_gradient(pos, c1, c2):
    """
    linear interpolation to generate colors

    pos: number
      a real number in [0, 1]
    c1: Tuple[number, number, number]
      start color
    c2: Tuple[number, number, number]
      end color
    """
    return (__linear_gradient(pos, c1[0], c2[0]), __linear_gradient(pos, c1[1], c2[1]), __linear_gradient(pos, c1[2], c2[2]))


import cv2
from src import util

#### the parameter used to generate output video
video_fourcc = cv2.VideoWriter_fourcc(*'mp4v') # video type
fps = 30 # output FPS
output_video = None # the opencv video write object
output_file = 'result.mp4' # when output_file is set to None, just preview and does not generate video file

max_seq = len(test)
seq = 0
for i in test:
    seq += 1
    img = i['frame']
    hand = i['hand']
    
    # normalize handpos for prediction
    x = normalize_hand(hand)

    # draw handpose in canvas
    canvas = img.copy()
    canvas = util.draw_handpose(canvas, [hand])

    # get the size of canvas to initialize video writer (if not initialized)
    rect = (0, 0, canvas.shape[1], canvas.shape[0])
    if output_video is None and output_file is not None:
        output_video = cv2.VideoWriter(output_file, video_fourcc, fps, (canvas.shape[1], canvas.shape[0]))

    # do the prediction
    with torch.no_grad():
        predicted = net(torch.from_numpy(x).float().unsqueeze_(0)).squeeze() # perform prediction
        predicted = F.softmax(predicted, 0) # calculate probability
        predicted_class = predicted.argmax() # retrive the predicted label
        predicted_prob = predicted[predicted_class] # retrive the prediction probability
        color = linear_gradient((predicted_prob.item() - 0.5) / 0.4, (31, 15, 197), (14, 161, 19)) # generate overlay text color

        # generate overlay text
        output = '{}, prob={:.0%}'.format(Y_map[predicted_class], predicted_prob)
        if predicted_prob <= 0.5:
            output = 'Undetermined'

        # render overlay text
        cv2.putText(canvas, output, (rect[0] + 10, rect[3] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1., color, 2, cv2.LINE_AA)

    print('Frame {}/{}'.format(seq, max_seq), end='  \r', flush=True)

    # write frame into file or preview it
    if output_video is not None:
        output_video.write(canvas)
    else:
        cv2.imshow('img', canvas)
        if cv2.waitKey(1) == 27:
            break

# cleaning up
cv2.destroyAllWindows()
if output_video is not None:
    output_video.release()
print('')
