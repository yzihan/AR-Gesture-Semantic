#!/usr/bin/env python

import numpy as np
import pickle
import torch
from torch import nn
from torch import optim
from torch.nn import functional as F

def load_dataset(fn):
    """
    Load the dataset from "fn" (generated by mkdataset.py and modified by gethandpos.py)

    returns the list of all data
    """

    # read index file
    with open(fn + '.index.pickle', 'rb') as f:
        index = pickle.load(f)

    # load part data according to index file
    data = []
    for item in index:
        # read part data ...
        with open(item['fn'], 'rb') as f:
            part_data = pickle.load(f)
        # ... and validate its correctness
        if len(part_data) != item['len']:
            print('Warning: incorrect index file: len({}) != {}'.format(item['fn'], item['len']))
        
        # load all data into exactly one list
        data.extend(part_data)
    return data

# load trainset, note that the trainset have been shuffled (in mkdataset.py)
train = load_dataset('trainset')

print('train:', len(train))

# seperate train and test for cross validation during training (train:test = 4:1, leave-one-out method)
cv_split_point = int(len(train) * 0.8)
train, cv_test = train[:cv_split_point], train[cv_split_point:]

print('cv_train:', len(train))
print('cv_test:', len(cv_test))

def normalize_hand(hand):
    """
    Normalize detected hand keypoints, moving the center (mean of coord) of non-zero points to (0,0)
    """
    retv = hand.astype(np.float)
    nonzero = (retv == 0).sum(1) != 2
    retv[nonzero] -= np.average(retv[nonzero], 0)
    return retv

def batch_dataset(d, BATCH_SIZE, has_Y, Y_map = None):
    """
    batch the data in dataset to enjoy a faster training

    d: the list of data (list of the dict describe the frame)
    BATCH_SIZE: a number
    has_Y: bool, does "d" contains ground-truth label. if so, we must generate Y_map, which maps labels (in str) into ID (a number).

    """
    X = []
    if has_Y:
        Y = []
        # get the set of all tags in this dataset
        new_map = set(map(lambda v: v['tag'], d))
        if Y_map is not None:
            # append into existed mapping, or ...
            Y_map.extend(new_map - set(Y_map))
        else:
            # ... generate a new mapping
            Y_map = list(new_map)

    # iterate over the whole dataset with steps to batch the data
    for i in range(0, len(d), BATCH_SIZE):
        # load data
        part_data = d[i: i + BATCH_SIZE]

        # bundle hand data (after normalization)
        X.append(torch.tensor(list(map(lambda v: normalize_hand(v['hand']), part_data)), dtype=torch.float32))

        # mapping string-form labels into numbered ID
        if has_Y:
            Y.append(torch.tensor(list(map(lambda v: Y_map.index(v['tag']), part_data))))

    if has_Y:
        return X, Y, Y_map
    else:
        return X


import torchvision.models as models

class mynet(nn.Module):
    """
    The naive FCN model
    """
    def __init__(self, shp, class_num):
        super(mynet, self).__init__()
        self.thenet = nn.Sequential(
            nn.Linear(np.product(shp), 128), nn.ReLU(True),
            nn.Linear(128, 256), nn.ReLU(True),
            nn.Linear(256, 72), nn.ReLU(True),
            nn.Linear(72, class_num),
            )
    def forward(self, X):
        v = X.flaten(start_dim=1)
        v = self.thenet(v)
        return v


#### Training parameters
EPOCH = 20 # count of epoch to train
BATCH_SIZE = 256 # the batch size of input data
X, Y, Y_map = batch_dataset(train, BATCH_SIZE, True) # generate train set
cv_X, cv_Y, Y_map = batch_dataset(cv_test, BATCH_SIZE, True, Y_map) # generate validate set

#### initialize the model and optimizer
# net = models.densenet161()
net = mynet(X[0].shape[1:], len(Y_map))
loss = nn.CrossEntropyLoss()
opt = optim.Adagrad(net.parameters(), lr = 0.01)

# calculate statistics of the count of each label, to calculate balanced-accuracy
groundtruth_counter = [ np.sum([ y.eq(i).sum() for y in cv_Y ]) for i in range(len(Y_map)) ]

for i in range(1, EPOCH + 1):
    k = 0 # loss during train
    p = 0 # loss during test

    for x,y in zip(X, Y):
        # the training
        predicted = net(x) # predict
        loss_val = loss(predicted, y) # calculate loss
        opt.zero_grad() # setup optimizer
        loss_val.backward() # backward
        opt.step() # step optimizer

        # accumulate loss during training
        k += loss_val.item() * x.shape[0]

    # validation on cv_test
    counter = [ 0 for i in range(len(Y_map)) ] # counter for each label, to calculate balanced-accuracy
    correct = 0 # counter for correct prediction
    with torch.no_grad():
        for x,y in zip(cv_X, cv_Y):
            predicted = net(x) # prediction
            loss_val = loss(predicted, y) # calculate the loss
            p += loss_val.item() * x.shape[0] # accumulate the loss

            predicted_class = predicted.argmax(1) # get prediction result

            predicted_success = predicted_class == y # construct the bool-array representing is_prediction_success
            for y_val in range(len(Y_map)):
                counter[y_val] += np.logical_and(predicted_success, y == y_val).sum() # calculate and accumulate statistics

            correct += predicted_success.sum() # accumulate currect prediction

    # calculate metrics
    k /= len(train)
    p /= len(cv_test)
    acc = 1. * correct / len(cv_test) # accuracy
    bal_acc = np.sum([ 1. * counter[x] / groundtruth_counter[x] for x in range(len(Y_map)) ]) / len(Y_map) # balanced accuracy
    print('epoch {}/{}: train loss={:.8F} test loss={:.8F} accuracy={:.5%} bal_acc={:.5%}'.format(i, EPOCH, k, p, acc, bal_acc))

# save the model
torch.save({
    'opt': opt.state_dict(),
    'net': net.state_dict(),
    'Y_map': Y_map,
    'epoch': EPOCH,
    }, 'train-result.pth')

print('Model saved to {}'.format('train-result.pth'))
